# dbutils.secrets.listScopes()
# dbutils.secrets.list("DatabricksScope")
%md
## Part 1
##### I've never used S3 so used Azure instead
##### Script this process so the files in the S3 bucket (Azure container) are kept in sync with the source when data on the website is updated, added, or deleted.
############################   configure ADLS storage
fileSystem = 'bls'
storageAccountName = dbutils.secrets.get(scope = "DatabricksScope", key = "StorageAccountName")
storageAccountKey = dbutils.secrets.get(scope = "DatabricksScope", key = "StorageAccountKey")
fullStorageAccountName =  "{0}.dfs.core.windows.net".format(storageAccountName)
# file_name = "pr.data.0.Current.txt"


#config storage accounts 
spark.conf.set(f"fs.azure.account.key.{storageAccountName}.dfs.core.windows.net", storageAccountKey)
adlsbasepath = f"abfss://{fileSystem}@{storageAccountName}.dfs.core.windows.net"
file_paths = adlsbasepath + "/input/"

# path_to_checkpoint = adlsbasepath + "/checkpoint/" + file_name
dbutils.fs.ls(adlsbasepath)
%md
#### Part 1.1 Retrieve files dynamically from bls.gov site. Use User-Agent header to avoid 403 errors
import requests
import re

# BLS FTP base URL
bls_base_url = "https://download.bls.gov/pub/time.series/pr/"

### original file names - see below for scraping website to get dynamically
# files = [
#     "pr.class", "pr.contacts", "pr.data.0.Current", "pr.data.1.AllData",
#     "pr.duration", "pr.footnote", "pr.measure", "pr.period",
#     "pr.seasonal", "pr.sector", "pr.series", "pr.txt"
# ]

# User-Agent header to avoid 403 errors
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def get_file_list(bls_base_url,headers):
    """Extract file names from the plain-text BLS directory listing."""
    response = requests.get(bls_base_url, headers=headers)
    response.raise_for_status()

    ### use regex to find files beginning with pr
    return re.findall(r'pr\.[\w\.]+', response.text)

files = get_file_list(bls_base_url,headers)
# print(files)

adls_uri = adlsbasepath + "/input/"

def stream_to_adls(file_name):
    url = bls_base_url + file_name
    try:
        response = requests.get(url, headers=headers)
        # print(response)
        response.raise_for_status()
        lines = response.text.splitlines()

        # Convert to Spark DataFrame
        df = spark.createDataFrame([(line,) for line in lines], ["value"])

        # Write to ADLS
        df.write.mode("overwrite") \
        .option("header", "false") \
        .option("delimiter","\t") \
        .csv(f"{adls_uri}/{file_name}") 
        print(f"✅ Retrieved: {file_name}")  ### I got this part from copilot hee hee
    except Exception as e:
        print(f"❌ Failed to retrieve {file_name}: {e}")

# Run for all files in files
for file in files:
    stream_to_adls(file)
%md
#### There's nothing I can find from bls that identifies a change in files so use autoloader for the syncing part; will use input folder in place of raw, delta folder for delta tables, schemas mandatory. This can then go on a schedule of some type to check bls files. Also some files have headers and some don't.....
from pyspark.sql.types import *

#### to determine schema for file
def get_bls_schema(file_name):
    """Return the appropriate schema for a given BLS file."""
    schemas = {
        "pr.data.1.AllData": StructType([
            StructField("series_id", StringType()),
            StructField("year", IntegerType()),
            StructField("period", StringType()),
            StructField("value", StringType()),
            StructField("footnote_codes", StringType())
        ]),
        "pr.data.0.Current": StructType([
            StructField("series_id", StringType()),
            StructField("year", IntegerType()),
            StructField("period", StringType()),
            StructField("value", DoubleType()),
            StructField("footnote_codes", StringType())
        ]),
        "pr.series": StructType([
            StructField("series_id", StringType()),
            StructField("area_code", StringType()),
            StructField("measure_code", StringType()),
            StructField("seasonal", StringType()),
            StructField("industry_code", StringType()),
            StructField("occupation_code", StringType()),
            StructField("class_code", StringType())
        ]),
        "pr.measure": StructType([
            StructField("measure_code", StringType()),
            StructField("measure_text", StringType())
        ]),
        "pr.class": StructType([
            StructField("class_code", StringType()),
            StructField("class_text", StringType())
        ]),
        "pr.duration": StructType([
            StructField("duration_code", StringType()),
            StructField("duration_text", StringType())
        ])
    }
    return schemas.get(file_name, None)
%md
##### some files didn't correspond to any schemas. In real life....I would investigate further.
##### But the files we need for subsequent steps have schemas so I just kept going.....
from pyspark.sql.types import *

#### function that takes file_name and schema and writes to delta table, triggers once
def autoload_bls_file(file_name, schema):
    input_path = adlsbasepath + f"/input/{file_name}"
    checkpoint_path = adlsbasepath + "/checkpoints/bls/{file_name}"
    delta_table_name = f"bls_{file_name.replace('.', '_').lower()}"
    delta_path = adlsbasepath + f"/delta/{delta_table_name}"

    df = (
    spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "csv")
    .option("cloudFiles.schemaLocation", adlsbasepath + "/schemas/bls/" + file_name)
    .option("delimiter", "\t")
    .option("header", "false")
    .schema(schema)  # Use .schema(), not .option("schema", ...)
    .load(input_path)
)
    query = df.writeStream \
        .format("delta") \
        .outputMode("append") \
        .option("checkpointLocation", checkpoint_path) \
        .trigger(availableNow=True) \
        .start(delta_path)

    query.awaitTermination()

    print(f"✅ Auto Loader triggered for {file_name} → {delta_table_name}")

### run for each file/schema

for file_name in files:
    schema = get_bls_schema(file_name)
    if not schema:
        print(f"No schema defined for file: {file_name}")  
    else:
        autoload_bls_file(file_name, schema)

# for stream in spark.streams.active:
#     stream.stop()
# dbutils.fs.rm(adlsbasepath + "/checkpoints", recurse=True)
# dbutils.fs.rm(adlsbasepath+ "/schemas/bls", recurse=True)
%md
#  Part 2
##### read from API and save population data in S3 (Azure blob) as a json file
##### save in: output/population.json
##### did this first and then made an azure function out of it for the terraform script......
##### This was working GREAT....and then it started throwing the 404 error.....
import requests
import json
import pandas as pd

population_url = "https://datausa.io/api/data?drilldowns=Nation&measures=Population"

try:
    response = requests.get(population_url)
    response.raise_for_status()
    population_data = response.json()
    population_string = json.dumps(population_data) ### make a string out of the json object
    
    # Convert to list of dicts
    population_list = [json.loads(population_string)]

    # Create pandas DataFrame
    population_pdf = pd.DataFrame(population_list)

    #### create spark df; write to blob as json file
    population_df = spark.createDataFrame(population_pdf)
    population_df.write.mode("overwrite").format("json").save(adlsbasepath + "/output/population.json")
except Exception as e:
    print(f"Error: {e}")


%md
# Part 3
##### Read pr.data.0.Current and json file into dataframes
##### from population data, generate the mean and the standard deviation of the annual US population across the years [2013, 2018] inclusive. Mean = sum of values divided by number of values. 
##### using stddev() function for standard deviation
##### nothing says: don't use tempviews, so I used tempviews after making dfs and wrote SQL
##### delimiter/schema not working after adding autoloader above to "automate" the reading of the files
##### parsed the data in the next cell.....
spark.read.text(adlsbasepath + "/input/pr.data.0.Current").show(truncate=False)

##### read data into dataframes
from pyspark.sql import functions as F
from pyspark.sql.functions import col, regexp_replace

population_df = spark.read.format("json").load(adlsbasepath + "/output/population.json")
population_df.createOrReplaceTempView("population_tv")

#### after adding the autoloader above...input files needed parsing...this should be investigated

df_raw = spark.read.text(adlsbasepath + "/input/pr.data.0.Current")
df_split = df_raw.selectExpr(
    "split(value, '\\t')[0] as series_id",
    "int(split(value, '\\t')[1]) as year",
    "split(value, '\\t')[2] as period",
    "trim(split(value, '\\t')[3]) as value",
    "trim(split(value, '\\t')[4]) as footnote_codes"
)

# Remove quotes from all string columns
df_clean = df_split.select([
    regexp_replace(col(c), '"', '').alias(c) if df_split.schema[c].dataType.simpleString() == 'string' else col(c)
    for c in df_split.columns
])

#### remove spaces from column names
bls_clean_df = df_clean.select([F.col(c).alias(c.strip()) for c in df_clean.columns])
bls_clean_df.createOrReplaceTempView("bls_current_tv")
%sql
SELECT SUM(item.population) / 6 AS populationMean
-- , AVG(item.population) AS populationAvg
, stddev(item.population) AS populationStdDev
FROM population_tv
JOIN LATERAL explode(data) AS t(item)
WHERE item.`ID Year` BETWEEN 2013 AND 2018
%md
#####  Part 3.2 For every series_id, find the best year: the year with the max/largest sum of "value" for all quarters in that year. (all quarters in that year is the same thing as all for that year. Unless you have funny calendar math and a quarter straddles two years....) Generate a report with each series id, the best year for that series, and the summed value for that year. Made a table report.
%sql
;with cte_maxvalue as (
SELECT series_id, year, SUM(value) AS valueSum
FROM bls_current_tv
GROUP BY  series_id, year
)
SELECT TRIM(b.series_id) AS series_id
, year
, b.maxValue AS value
FROM (
SELECT series_id, MAX(valueSum) AS maxValue
FROM (
SELECT series_id, year, SUM(value) AS valueSum
FROM bls_current_tv
-- WHERE trim(series_id) = 'PRS30006011'
GROUP BY  series_id, year
)a
GROUP BY a.series_id
)b
INNER JOIN
( 
SELECT *
FROM cte_maxvalue) cte
ON TRIM(cte.series_id)= TRIM(b.series_id)
AND cte.valueSum = b.maxValue
ORDER BY cte.series_id, cte.year

----series_id, year, period,value,footnote_codes
# %sql
# SELECT *
# FROM bls_current_tv
# WHERE TRIM(series_id) = 'PRS30006032'
# AND TRIM(period) = 'Q01'
# AND TRIM(year) = '2018'

# returns different values than what's in git repo.....but works for data that's there now
%md
#####  Part 3.3 generate a report that will provide the value for series_id = PRS30006032 and period = Q01 and the population for that given year -- so year is a parameter? Added parameter year and to WHERE clause. Also one row doesn't make a very good report; made a table report and also a dashboard via the icon above
##### See dashboard report that shows the table with values for series_id = PRS30006032 and period = Q01 and the population, where available
%sql
;with cte_population AS (
SELECT TRIM(item.`ID Year`) AS year, item.population
FROM population_tv
JOIN LATERAL explode(data) AS t(item)
)
SELECT series_id, cte.year, period, value, population
FROM (
SELECT TRIM(series_id) AS series_id, TRIM(year) AS year,period, value
FROM bls_current_tv
WHERE TRIM(series_id) = 'PRS30006032'
AND TRIM(period) = 'Q01'
) a 
INNER JOIN cte_population cte 
ON a.year = cte.year
AND cte.year = :year  --- takes year parameter
ORDER BY a.year
